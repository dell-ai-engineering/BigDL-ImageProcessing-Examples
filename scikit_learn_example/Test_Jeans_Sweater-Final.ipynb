{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bigdl.nn.criterion import CrossEntropyCriterion\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "from zoo.common.nncontext import *\n",
    "from zoo.feature.image import *\n",
    "from zoo.pipeline.api.keras.layers import Dense, Input, Flatten\n",
    "from zoo.pipeline.api.keras.models import *\n",
    "from zoo.pipeline.api.net import *\n",
    "from zoo.pipeline.nnframes import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn.criterion import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, StringType, ArrayType\n",
    "from zoo.common.nncontext import *\n",
    "from zoo.feature.image import *\n",
    "from zoo.pipeline.api.keras.layers import Input, Flatten, Dense\n",
    "from zoo.pipeline.api.keras.models import *\n",
    "from zoo.pipeline.api.net import *\n",
    "from zoo.pipeline.nnframes import *\n",
    "from zoo.feature.image.imagePreprocessing import *\n",
    "\n",
    "import random\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from zoo.pipeline.api.keras.metrics import AUC\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pyspark.ml import Pipeline\n",
    "from bigdl.optim.optimizer import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf().setAppName(\"testMutipleLabels\")\n",
    "sc = init_nncontext(sparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = create_spark_conf().setAppName(\"testMutipleLabels\")\n",
    "sc = init_nncontext(sparkConf)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/mahmood/analytics-zoo/apps/dogs-vs-cats/JeansDatasets/red_shirt/\"\n",
    "print path\n",
    "new_path=\"/home/mahmood/analytics-zoo/apps/dogs-vs-cats/JeansDatasets/allJeansImage/\"\n",
    "print new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.util.common import *\n",
    "from bigdl.transform.vision.image import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually set model_path and image_path for training\n",
    "\n",
    "1. model_path = path to the pre-trained models. (E.g. path/to/model/bigdl_inception-v1_imagenet_0.4.0.model)\n",
    "\n",
    "2. image_path = path to the folder of the training images. (E.g. path/to/data/dogs-vs-cats/demo/\\*/\\*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_path = \"/home/mahmood/analytics-zoo/demo/bigdl_inception-v1_imagenet_0.4.0.model\"\n",
    "\n",
    "#image_path = \"file:///home/mahmood/analytics-zoo/demo/*/*\"\n",
    "#image_path=ImageFrame.read(\"hdfs:///datasets/xray/train/*\", sc, 2) this does not work \n",
    "#image_path=ImageFrame.read(\"hdfs://hinfra/datasets/xray/train/*\",sc,2)\n",
    "#image_path = \"file:///home/mahmood/Downloads/*/*\"\n",
    "#image_path = \"file:///home/mahmood/analytics-zoo/demo/*/*\"\n",
    "image_path = \"file:///home/mahmood/analytics-zoo/apps/dogs-vs-cats/JeansDatasets/allJeansImage/\"\n",
    "imageDF = NNImageReader.readImages(image_path, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "txtfiles =[]\n",
    "Image_index=[]\n",
    "for file in glob(\"/home/mahmood/analytics-zoo/apps/dogs-vs-cats/JeansDatasets/allJeansImage/*\"):\n",
    "    txtfiles.append(file.split('/'))\n",
    "for i in range(len(txtfiles)):  \n",
    "    Image_index.append(txtfiles[i][8:9])\n",
    "    \n",
    "print len(txtfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels1=[]\n",
    "for j in range(len(Image_index)):\n",
    "    s=re.sub('[^A-Za-z_.]', '', str(Image_index[j]))\n",
    "    #print s\n",
    "    s=s.strip('_.jpg')\n",
    "    s=s.strip('_.jpeg')\n",
    "    s=s.strip('_.JPG')\n",
    "    s=s.strip('_.pn')\n",
    "    Labels1.append(s)\n",
    "Labels1[0:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=Image_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_indx=[]\n",
    "for j in range(len(Image_index)):\n",
    "    s=re.sub('[^A-Za-z0-9_.]', '', str(Image_index[j]))\n",
    "    #print s\n",
    "    \n",
    "    Img_indx.append(s)\n",
    "Img_indx[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import izip\n",
    "\n",
    "with open('AllLabels.csv', 'wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(izip(Img_indx,Labels1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# open file\n",
    "with open('AllLabels.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    " \n",
    "    # read file row by row\n",
    "    for row in reader:\n",
    "        print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path='/home/mahmood/analytics-zoo/apps/dogs-vs-cats/JeansDatasets/allJeansImage/'\n",
    "label_path = '/home/mahmood/analytics-zoo/apps/dogs-vs-cats/AllLabels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLabel = udf(lambda x: text_to_label(x), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labelDF=pd.read_csv(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlCtx = SQLContext(sc)\n",
    "labelDFsp = sqlCtx.createDataFrame(labelDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_texts = list(\"\"\"black,jeans,blue,dress,shirt,red\"\"\".replace(\"\\n\", \"\").split(\",\"))\n",
    "print label_texts\n",
    "\n",
    "label_map = {k: v for v, k in enumerate(label_texts)}\n",
    "print label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_label(text):\n",
    "    arr = [0.0] * len(label_texts)\n",
    "    for l in text.split(\"_\"):\n",
    "        arr[label_map[l]] = 1.0\n",
    "     \n",
    "    return arr\n",
    "print text_to_label(\"black_jeans\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLabel = udf(lambda x: text_to_label(x), ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDF11 = labelDFsp.select(\"Image_index\", \"labels\") \\\n",
    "   .withColumn(\"label\", getLabel(col('labels')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDF11.select(\"Image_index\",\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getLabel = udf(lambda x: text_to_label(x), ArrayType(DoubleType()))\n",
    "getName = udf(lambda row: os.path.basename(row[0]), StringType())\n",
    "imageDF = NNImageReader.readImages(image_path, sc, resizeH=300, resizeW=300 ) \\\n",
    "    .withColumn(\"Image_index\", getName(col('image')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labelDF = spark.read.load(label_path, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\") \\\n",
    "   .select(\"Image Index\", \"Finding Labels\") \\\n",
    "   .withColumn(\"label\", getLabel(col('Finding Labels')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageDF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MY code start from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label_texts = list(\"\"\"Atelectasis, Consolidation, Infiltration, Pneumothorax, Edema, Emphysema, Fibrosis, Effusion, Pneumonia, Pleural_Thickening, Cardiomegaly, Nodule, Mass, Hernia, No Finding\"\"\".replace(\"\\n\", \"\").split(\", \"))\n",
    "label_map = {k: v for v, k in enumerate(label_texts)}\n",
    "\n",
    "#-------------------------------------------\n",
    "def text_to_label(text):\n",
    "    arr = [0.0] * len(label_texts)\n",
    "    \n",
    "    for l in text.split(\",\"):\n",
    "        arr[label_map[l]] = 1.0\n",
    "     \n",
    "    return arr\n",
    "#------------------------------------------\n",
    "getLabel = udf(lambda x: text_to_label(x), ArrayType(DoubleType()))\n",
    "getName = udf(lambda row: os.path.basename(row[0]), StringType())\n",
    "imageDF = NNImageReader.readImages(image_path, sc, resizeH=256, resizeW=256) \\\n",
    "    .withColumn(\"Image Index\", getName(col('image')))\n",
    "labelDF = spark.read.load(label_path, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\") \\\n",
    "   .select(\"Image Index\", \"Finding Labels\") \\\n",
    "   .withColumn(\"label\", getLabel(col('Finding Labels')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = imageDF.join(labelDF11, on=\"Image_index\", how=\"inner\")\n",
    "(trainingDF, validationDF) = train_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingDF, validationDF) = labelDF.randomSplit([0.9, 0.1])\n",
    "labelDF.select(\"name\",\"label\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type(trainingDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fine-tune a pre-trained model by removing the last few layers, freezing the first few layers, and adding some new layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ChainedPreprocessing(\n",
    "        [RowToImageFeature(), ImageResize(256, 256), ImageCenterCrop(224, 224),\n",
    "         ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(), ImageFeatureToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Net API to load a pre-trained model, including models saved by Analytics Zoo, BigDL, Torch, Caffe and Tensorflow. Please refer to [Net API Guide](https://analytics-zoo.github.io/master/#APIGuide/PipelineAPI/net/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = Net.load_bigdl(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the last few layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print all the model layers and you can choose which layer(s) to remove.\n",
    "\n",
    "When a model is loaded using Net, we can use the newGraph(output) api to define a Model with the output specified by the parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in full_model.layers:\n",
    "    print (layer.name())\n",
    "model = full_model.new_graph([\"pool5/drop_7x7_s1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returning model's output layer is \"pool5/drop_7x7_s1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze some layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We freeze layers from input to pool4/3x3_s2 inclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.freeze_up_to([\"pool4/3x3_s2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputNode = Input(name=\"input\", shape=(3, 224, 224))\n",
    "inception = model.to_keras()(inputNode)\n",
    "flatten = Flatten()(inception)\n",
    "logits = Dense(6, activation=\"sigmoid\")(flatten)\n",
    "lrModel = Model(inputNode, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a few new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_summary = ValidationSummary(log_dir=\"/home/mahmood/analytics-zoo/apps/dogs-vs-cats/logDirectroy/log\", app_name=\"testMutipleLabels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputNode = Input(name=\"input\", shape=(3, 224, 224))\n",
    "inception = model.to_keras()(inputNode)\n",
    "flatten = Flatten()(inception)\n",
    "logits = Dense(2)(flatten)\n",
    "lrModel = Model(inputNode, logits)\n",
    "classifier = NNClassifier(lrModel, CrossEntropyCriterion(), transformer) \\\n",
    "        .setLearningRate(0.003).setBatchSize(24).setMaxEpoch(1).setFeaturesCol(\"image\") \\\n",
    "        .setCachingSample(False)\n",
    "pipeline = Pipeline(stages=[classifier])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NNEstimator(lrModel, MultiLabelSoftMarginCriterion(), transformer, SeqToTensor([6])) \\\n",
    "     .setLearningRate(0.001).setBatchSize(24).setMaxEpoch(10).setFeaturesCol(\"image\")\\\n",
    "     .setCachingSample(False)\\\n",
    "     .setValidation(EveryEpoch(), validationDF,[AUC()],  24)\n",
    "#\\\n",
    " #    .setValidationSummary(val_summary)\n",
    "#\\\n",
    "    # .setCheckpoint(\"/home/mahmood/analytics-zoo/apps/dogs-vs-cats/logDirectroy/checkpoint\", EveryEpoch(),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnModel = classifier.fit(trainingDF)\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnModel.transform(trainingDF).show(5)\n",
    "\n",
    "predictionDF = nnModel.transform(validationDF).cache()\n",
    "\n",
    "predictionDF.select(\"Image_index\",\"label\",\"prediction\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
