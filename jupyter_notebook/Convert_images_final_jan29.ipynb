{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert X-Ray images into SQL Spark Dataframe \n",
    "In this notebook, Chest X-Ray dataset has 112120 gray scale images with 1024 by 1024 image size. \n",
    "The dataset contains x-ray images that shows one or more Thorax Disease and the total number of disease is 14. This make the problem as multiple class and multiple lables problem. \n",
    "The volume of this dataset is around 42 GB.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, all images will be converted into SQL spark dataframe and will be saved in HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, import all the required packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn.criterion import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from zoo.common.nncontext import *\n",
    "from zoo.pipeline.nnframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate one hot encoding \n",
    "This part of the code generates SQL spark dataframe for the mutiple labels for each class as one hot encoding then save that as a CSV file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_length = 14\n",
    "label_texts = [\"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Infiltration\", \"Mass\", \"Nodule\", \"Pneumonia\", \"Pneumothorax\",\n",
    "               \"Consolidation\", \"Edema\", \"Emphysema\", \"Fibrosis\", \"Pleural_Thickening\", \"Hernia\"]\n",
    "label_map = {k: v for v, k in enumerate(label_texts)}\n",
    "\n",
    "def write_to_csv(df, label_col=\"label\"):\n",
    "    for i in range(label_length):\n",
    "        get_Kth = udf(lambda a: a[i] * (i + 1), DoubleType())\n",
    "        df = df.withColumn(str(i) + \" th\", get_Kth(col(label_col)))\n",
    "\n",
    "    df.show()\n",
    "    df = df.drop(\"label\")\n",
    "    df.write.csv(\"label.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class distribution \n",
    "This functions computes the number of images in each class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_for_kth_class(k, df, label_col=\"label\"):\n",
    "    print(k, \"th class distribution \\t (\", label_texts[k], \") is:\")\n",
    "    get_Kth = udf(lambda a: a[k], DoubleType())\n",
    "    extracted_df = df.withColumn(\"kth_label\", get_Kth(col(label_col)))\n",
    "    extracted_df.groupby(\"kth_label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images and their labels into SQL spark dataframe \n",
    "This part read all images and their labels ( as CSV ) and generate SQL saprk dataframe. A new label column will be added to the dataframe. The resulted dataframe is saved in HDFS as TestDF and TrainDF. It also display the number of images in each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image_path =\"hdfs:///datasets/xray_files/xray/all_images\" #sys.argv[1] #\"/home/yuhao/workspace/data/xray/middle_images\"\n",
    "    label_path =/datasets/xray_files/ #sys.argv[2] #\"/home/yuhao/workspace/data/xray/Data_Entry_2017.csv\"\n",
    "    save_path = \"hdfs:///datasets/xray_files/DataFrames\" #sys.argv[3] #\"./save_model\"\n",
    "\n",
    "    sparkConf = create_spark_conf().setAppName(\"test_dell_x_ray\")\n",
    "    sc = init_nncontext(sparkConf)\n",
    "    spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "    print(sc.master)\n",
    "\n",
    "    def text_to_label(text):\n",
    "        arr = [0.0] * len(label_texts)\n",
    "        for l in text.split(\"|\"):\n",
    "            if l != \"No Finding\":\n",
    "                arr[label_map[l]] = 1.0\n",
    "        return arr\n",
    "\n",
    "    getLabel = udf(lambda x: text_to_label(x), ArrayType(DoubleType()))\n",
    "    getName = udf(lambda row: os.path.basename(row[0]), StringType())\n",
    "    imageDF = NNImageReader.readImages(image_path, sc, resizeH=256, resizeW=256, image_codec=1) \\\n",
    "        .withColumn(\"Image_Index\", getName(col('image')))\n",
    "    imageDF=imageDF.withColumnRenamed('Image_Index', 'Image_Index')\n",
    "    labelDF = spark.read.load(label_path + \"/Data_Entry_2017.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\") \\\n",
    "        .select(\"Image_Index\", \"Finding_Labels\") \\\n",
    "        .withColumn(\"label\", getLabel(col('Finding_Labels'))) \\\n",
    "        .withColumnRenamed('Image_Index', 'Image_Index') \\\n",
    "        .select(\"Image_Index\", \"label\")\n",
    "    labelDF.printSchema()\n",
    "    train_df = imageDF.join(labelDF, on=\"Image_Index\", how=\"inner\")\n",
    "\n",
    "    trainingList = spark.read.text(label_path + \"/train_val_list.txt\").withColumnRenamed(\"value\", \"Image_Index\")\n",
    "    testList = spark.read.text(label_path + \"/test_list.txt\").withColumnRenamed(\"value\", \"Image_Index\")\n",
    "\n",
    "    trainingDF = train_df.join(trainingList, on=\"Image_Index\")\n",
    "    testDF = train_df.join(testList, on=\"Image_Index\")\n",
    "\n",
    "    trainingDF.write.save(save_path + \"/trainingDFjan29\")\n",
    "    testDF.write.save(save_path + \"/testDF\")\n",
    "\n",
    "    print(\"data saved at \", save_path)\n",
    "\n",
    "    loadedTrainingDF = spark.read.load(save_path + \"/trainingDFjan29\")\n",
    "    loadedTestDF = spark.read.load(save_path + \"/testDFjan29\")\n",
    "    print(\"trainingDF count: \", loadedTrainingDF.count())\n",
    "    print(\"testDF count: \", loadedTestDF.count())\n",
    "    loadedTrainingDF.show()\n",
    "    loadedTestDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
