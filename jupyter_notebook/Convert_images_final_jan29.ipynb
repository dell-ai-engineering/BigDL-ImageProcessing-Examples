{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert X-Ray images into SQL Spark Dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, all the ChestXray images will be converted into SQL spark dataframe and saved in HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, import all the required packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/scipy/sparse/lil.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _csparsetools\n"
     ]
    }
   ],
   "source": [
    "from bigdl.nn.criterion import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from zoo.common.nncontext import *\n",
    "from zoo.pipeline.nnframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate one hot encoding \n",
    "This part of the code generates SQL spark dataframe for the mutiple labels for each class as one hot encoding then save that as a CSV file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_length = 14\n",
    "label_texts = [\"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Infiltration\", \"Mass\", \"Nodule\", \"Pneumonia\", \"Pneumothorax\",\n",
    "               \"Consolidation\", \"Edema\", \"Emphysema\", \"Fibrosis\", \"Pleural_Thickening\", \"Hernia\"]\n",
    "label_map = {k: v for v, k in enumerate(label_texts)}\n",
    "\n",
    "def write_to_csv(df, label_col=\"label\"):\n",
    "    for i in range(label_length):\n",
    "        get_Kth = udf(lambda a: a[i] * (i + 1), DoubleType())\n",
    "        df = df.withColumn(str(i) + \" th\", get_Kth(col(label_col)))\n",
    "\n",
    "    df.show()\n",
    "    df = df.drop(\"label\")\n",
    "    df.write.csv(\"label.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the image and pre-trained model paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"hdfs:///path_to_your_downloaded_images\" #sys.argv[1] \n",
    "label_path = \"hdfs:///path_to_your_saved_label_and_txt_files(Data_Entry_2017.csv, train_val_list.txt, test_list.txt)\" #sys.argv[2] \n",
    "save_path = \"hdfs:///path_to_save_your_DataFrames(trainDF, testDF)\" #sys.argv[3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the spark session and read images then convert it into Spark Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yarn\n"
     ]
    }
   ],
   "source": [
    "sparkConf = create_spark_conf().setAppName(\"convertimgs_to_spdf\")\n",
    "sc = init_nncontext(sparkConf)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "print(sc.master)\n",
    "\n",
    "def text_to_label(text):\n",
    "    arr = [0.0] * len(label_texts)\n",
    "    for l in text.split(\"|\"):\n",
    "        if l != \"No Finding\":\n",
    "            arr[label_map[l]] = 1.0\n",
    "    return arr\n",
    "\n",
    "getLabel = udf(lambda x: text_to_label(x), ArrayType(DoubleType()))\n",
    "getName = udf(lambda row: os.path.basename(row[0]), StringType())\n",
    "imageDF = NNImageReader.readImages(image_path, sc, resizeH=256, resizeW=256, image_codec=1) \\\n",
    "    .withColumn(\"Image_Index\", getName(col('image')))\n",
    "imageDF=imageDF.withColumnRenamed('Image_Index', 'Image_Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading label CSV file and generate spark dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Image_Index: string (nullable = true)\n",
      " |-- Finding Labels: string (nullable = true)\n",
      " |-- label: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelDF = spark.read.load(label_path + \"/Data_Entry_2017.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\") \\\n",
    "        .select(\"Image Index\", \"Finding Labels\") \\\n",
    "        .withColumn(\"label\", getLabel(col('Finding Labels'))) \\\n",
    "        .withColumnRenamed('Image Index', 'Image_Index') \n",
    "       # .select(\"Image_Index\", \"label\")\n",
    "labelDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the images with their labels in a new  dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = imageDF.join(labelDF, on=\"Image_Index\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulate the train and test dataframe by reading their labels from train_val_list and test_list text files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingList = spark.read.text(label_path + \"/train_val_list.txt\").withColumnRenamed(\"value\", \"Image_Index\")\n",
    "testList = spark.read.text(label_path + \"/test_list.txt\").withColumnRenamed(\"value\", \"Image_Index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct train and test dataframe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDF = train_df.join(trainingList, on=\"Image_Index\")\n",
    "testDF = train_df.join(testList, on=\"Image_Index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the special character from column labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDF1=trainingDF.withColumnRenamed(\"Finding Labels\", \"Finding_Labels\")\n",
    "testDF1=testDF.withColumnRenamed(\"Finding Labels\", \"Finding_Labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save train and test dataframe in HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data saved at ', 'hdfs:///datasets/xray_files/DataFrames')\n"
     ]
    }
   ],
   "source": [
    "trainingDF1.write.save(save_path + \"/trainDF\") \n",
    "testDF1.write.save(save_path + \"/testDF\")\n",
    "print(\"data saved at \", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataframes and print the number of images in each of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('trainingDF count: ', 86524)\n",
      "('testDF count: ', 25596)\n",
      "+----------------+--------------------+--------------------+--------------------+\n",
      "|     Image_Index|               image|      Finding_Labels|               label|\n",
      "+----------------+--------------------+--------------------+--------------------+\n",
      "|00023287_000.png|[hdfs://pNameNode...|            Effusion|[0.0, 0.0, 1.0, 0...|\n",
      "|00023313_008.png|[hdfs://pNameNode...|       Effusion|Mass|[0.0, 0.0, 1.0, 0...|\n",
      "|00023450_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023477_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023543_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023650_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023708_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023731_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023775_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023861_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023864_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023873_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023883_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023888_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023941_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00023991_000.png|[hdfs://pNameNode...|Fibrosis|Pleural_...|[0.0, 0.0, 0.0, 0...|\n",
      "|00024037_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00024079_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00024081_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00024091_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+--------------------+--------------------+--------------------+\n",
      "|     Image_Index|               image|      Finding_Labels|               label|\n",
      "+----------------+--------------------+--------------------+--------------------+\n",
      "|00027758_002.png|[hdfs://pNameNode...|         Atelectasis|[1.0, 0.0, 0.0, 0...|\n",
      "|00028108_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028137_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028168_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028200_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028220_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028222_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028265_001.png|[hdfs://pNameNode...|         Mass|Nodule|[0.0, 0.0, 0.0, 0...|\n",
      "|00028280_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028318_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028366_001.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028371_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028382_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028383_002.png|[hdfs://pNameNode...|Effusion|Infiltra...|[0.0, 0.0, 1.0, 1...|\n",
      "|00028406_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028413_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028422_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028450_000.png|[hdfs://pNameNode...|          No Finding|[0.0, 0.0, 0.0, 0...|\n",
      "|00028454_000.png|[hdfs://pNameNode...|  Pleural_Thickening|[0.0, 0.0, 0.0, 0...|\n",
      "|00028454_012.png|[hdfs://pNameNode...|            Effusion|[0.0, 0.0, 1.0, 0...|\n",
      "+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loadedTrainingDF = spark.read.load(save_path + \"/trainDF\")\n",
    "loadedTestDF = spark.read.load(save_path + \"/testDF\")\n",
    "print(\"trainingDF count: \", loadedTrainingDF.count())\n",
    "print(\"testDF count: \", loadedTestDF.count())\n",
    "loadedTrainingDF.show()\n",
    "loadedTestDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
