{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn.criterion import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, StringType, ArrayType\n",
    "from zoo.common.nncontext import *\n",
    "from zoo.feature.image import *\n",
    "from zoo.pipeline.api.keras.layers import Input, Flatten, Dense\n",
    "from zoo.pipeline.api.keras.models import *\n",
    "from zoo.pipeline.api.net import *\n",
    "from zoo.pipeline.nnframes import *\n",
    "from zoo.feature.image.imagePreprocessing import *\n",
    "import random\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from bigdl.transform.vision.image import RandomTransformer, HFlip\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pyspark.ml import Pipeline\n",
    "from bigdl.optim.optimizer import * \n",
    "from zoo.pipeline.api.keras.metrics import AUC\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. READ IMAGE , Model and  CSV paths \n",
    "Comment the path of the model/dataset  that you do not need to use  it and make sure to use the right  path and choped the last layer of pre-traned model in the next cells:\n",
    "1. Inception model path \n",
    "2. Resnet50 model path \n",
    "3. test dataset path \n",
    "4. train dataset path \n",
    "5. validation dataset path \n",
    "6. CSV file with all image labels path \n",
    "\n",
    "All dataset are saved in HDFS and the paths return  as strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_Pathes(): \n",
    "    #model_path =\"hdfs:///datasets/xray_files/xray/analytics-zoo_resnet-50_imagenet_0.1.0.model\"\n",
    "    model_path =\"hdfs:///datasets/xray_files/xray/bigdl_inception-v1_imagenet_0.4.0.model\"\n",
    "    image_test_path =\"hdfs:///datasets/xray_files/xray/test\" \n",
    "    image_path =\"hdfs:///datasets/xray_files/xray/train\"\n",
    "    #image_path=\"hdfs:///datasets/xray_files/xray/all_images\"\n",
    "    #image_path=\"hdfs:///datasets/RGBresizedto256\"\n",
    "    #image_path=\"hdfs:///datasets/xray_files/RGB_PIL_Imge\"\n",
    "    label_path = \"hdfs:///datasets/xray_files/Data_Entry_2017.csv\"\n",
    "    return model_path,image_test_path,image_path,label_path   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path,image_test_path,image_path,label_path=Read_Pathes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of dataset  and model paths :\n",
    "Print all paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print model_path\n",
    "print image_test_path\n",
    "print image_path\n",
    "print image_test_path\n",
    "print label_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spark Engine Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = create_spark_conf().setAppName(\"testNNClassifer\")\n",
    "sc = init_nncontext(sparkConf)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a list of dataset labels and map it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_texts = list(\"\"\"Atelectasis, Consolidation, Infiltration, Pneumothorax, Edema, Emphysema, Fibrosis, Effusion, Pneumonia, Pleural_Thickening, Cardiomegaly, Nodule, Mass, Hernia, No Finding\"\"\".replace(\"\\n\", \"\").split(\", \"))\n",
    "label_map = {k: v for v, k in enumerate(label_texts)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Label binarizer function \n",
    "Create a multiple labels hot encoding for each class. There is 15 claases with mutiple labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_label(text):\n",
    "    arr = [0.0] * len(label_texts)\n",
    "    for l in text.split(\"|\"):\n",
    "        arr[label_map[l]] = 1.0\n",
    "     \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read all images as dataframe and merege them with thier labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLabel = udf(lambda x: text_to_label(x), ArrayType(DoubleType()))\n",
    "getName = udf(lambda row: os.path.basename(row[0]), StringType())\n",
    "imageDF = NNImageReader.readImages(image_path, sc, resizeH=256, resizeW=256, image_codec=1) \\\n",
    "    .withColumn(\"Image Index\", getName(col('image')))\n",
    "imageDF=imageDF.withColumnRenamed('Image Index', 'Image_Index')\n",
    "labelDF = spark.read.load(label_path, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\") \\\n",
    "   .select(\"Image Index\", \"Finding Labels\") \\\n",
    "   .withColumn(\"label\", getLabel(col('Finding Labels')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDF = labelDF.withColumnRenamed('Image Index', 'Image_Index')\\\n",
    "    .withColumnRenamed('Finding Labels', 'Finding_Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split dataset frame into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = imageDF.join(labelDF, on=\"Image_Index\", how=\"inner\")\n",
    "#(trainingDF1, validationDF1) = train_df.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingDF, validationDF) = train_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Drop the unused dataset in order to better memory utilization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.unpersist()\n",
    "trainingDF1.unpersist()\n",
    "validationDF1.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model definition and loading \n",
    "There is two models : inception and Resnet50 . Load on of them and layers can be freezed or unfreezed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_model = Net.load_bigdl(model_path)\n",
    "\n",
    "#for layer in full_model.layers:\n",
    "#    print (layer.name())\n",
    "#this is for Resnet50 \n",
    "#model = full_model.new_graph([\"pool5\"])\n",
    "#model.freeze_up_to([\"pool4/3x3_s2\"])\n",
    "# this for Inception v1\n",
    "model = full_model.new_graph([\"pool5/drop_7x7_s1\"])  # this inception \n",
    "inputNode = Input(name=\"input\", shape=(3, 224, 224))\n",
    "inception = model.to_keras()(inputNode)\n",
    "flatten = Flatten()(inception)\n",
    "logits = Dense(15, activation=\"sigmoid\")(flatten)\n",
    "lrModel = Model(inputNode, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.Set the validation sammary and save in order  to use it with tensorboard\n",
    "tensorboard --logdir=/home/mahmood/ChestXray/logDirectory/testNNClassifer/validation --port 8080\n",
    "\n",
    "tensorboard --logdir=/home/mahmood/ChestXray/logDirectory/testNNClassifer/validation --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logdir ='/logDirectory'\n",
    "train_summary = TrainSummary(log_dir=\"/home/mahmood/ChestXray/logDirectory\", app_name=\"testNNClassifer\")\n",
    "val_summary = ValidationSummary(log_dir=\"/home/mahmood/ChestXray/logDirectory\", app_name=\"testNNClassifer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Transform images in order to preprocess them to fit with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(1))\n",
    "train_summary.set_summary_trigger(\"LearningRate\", SeveralIteration(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_dir='/home/cdsw/checkPoint'\n",
    "# compose a pipeline that includes feature transform, pretrained model \n",
    "transformer = ChainedPreprocessing(\n",
    "    [RowToImageFeature(), ImageCenterCrop(224, 224), BigDLAdapter(RandomTransformer(HFlip(), 0.5)), \n",
    "     ImageChannelNormalize(123.68, 116.779, 103.939 ), ImageMatToTensor(), ImageFeatureToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_RGB=True  ,ImageResize(256, 256), ImageChannelNormalize(123.68, 116.779, 103.939, 58.395, 57.12, 57.375)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tmp_dir='/home/cdsw/checkPoint'\n",
    "# compose a pipeline that includes feature transform, pretrained model \n",
    "transformer = ChainedPreprocessing(\n",
    "    [RowToImageFeature(), ImageResize(256, 256),BigDLAdapter(RandomTransformer(HFlip(), 0.5)), \n",
    "     ImageChannelNormalize(mean_r=0.0, mean_g=0.0, mean_b=0.0, std_r=255.0, std_g=255.0, std_b=255.0 ), ImageMatToTensor(), ImageFeatureToTensor()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#I would like to try this \n",
    "transformer = ChainedPreprocessing([RowToImageFeature(), ImageResize(256, 256),\n",
    "                                    ImageCenterCrop(256, 256),\n",
    "                                    ImageChannelNormalize(123.0, 117.0, 104.0),\n",
    "                                    ImageMatToTensor(),\n",
    "                                    ImageFeatureToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ImageChannelNormalize(mean_r=0.0, mean_g=0.0, mean_b=0.0, std_r=255.0, std_g=255.0, std_b=255.0 ), ImageMatToTensor(), ImageFeatureToTensor(),ImageSetToSample()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.Estimator Defintion and setting the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NNEstimator(lrModel, MultiLabelSoftMarginCriterion(), transformer, SeqToTensor([15])) \\\n",
    "     .setLearningRate(0.005).setBatchSize(64).setMaxEpoch(8).setFeaturesCol(\"image\")\\\n",
    "     .setCachingSample(False)\\\n",
    "     .setValidation(EveryEpoch(), validationDF, [AUC()], 64)\\\n",
    "     .setTrainSummary(train_summary) \\\n",
    "     .setValidationSummary(val_summary) \\\n",
    "     .setCheckpoint(\"/home/mahmood/ChestXray/checkpoint\", EveryEpoch(), False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".setOptimMethod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(1))\n",
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnModel = classifier.fit(trainingDF)\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnModel.transform(trainingDF).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF = nnModel.transform(validationDF).cache()\n",
    "predictionDF.select(\"Image_Index\",\"label\",\"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF.select(\"Image_Index\",\"label\",\"prediction\").sort(\"label\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictionDF=predictionDF.withColumn('label', predictionDF['label'].cast(ArrayType(FloatType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF=predictionDF.withColumn('label', predictionDF['label'].cast(ArrayType(DoubleType())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. AUC  CALCULATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import seaborn as sns; sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Connvert label and prediction into 2D array to fit with AUC_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lab=predictionDF.select(\"label\").collect()\n",
    "\n",
    "L=np.array(Lab)\n",
    "\n",
    "L=L.reshape(L.shape[0],L.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre=predictionDF.select(\"prediction\").collect()\n",
    "\n",
    "\n",
    "Pre1=np.array(Pre)\n",
    "\n",
    "Pre1=Pre1.reshape(Pre1.shape[0],Pre1.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def toArray(dfColum)\n",
    "    P=np.array(dfColum)\n",
    "    P=P.reshape(P.shape[0],P.shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvertToArrayUdf = udf(lambda A: toArray(A), ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def array_fetch(P):\n",
    "  P=P.toPandas().values\n",
    "  array1=np.zeros((len(P),len(P[0][0][:])))\n",
    "  for i in range(len(P)):\n",
    "      for j in range(len(P[0][0][:])):\n",
    "          array1[i,j]=P[i][0][j]\n",
    "  return array1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab=predictionDF.select(\"label\")\n",
    "Pre=predictionDF.select(\"prediction\")\n",
    "def array_fetch(P):\n",
    "    P=np.array(P.collect())\n",
    "    P=P.reshape(P.shape[0],P.shape[2])\n",
    "    return P\n",
    "\n",
    "LabelArray=array_fetch(Lab)\n",
    "PredArray=array_fetch(Pre)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score=roc_auc_score(P, Pre1)\n",
    "n_classes=15\n",
    "\n",
    "print('total roc_auc_score : = {0}'.format(total_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Get AUC values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_values(LabelArray,PredArray): \n",
    "    n_classes=15\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(LabelArray[:, i], PredArray[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        print roc_auc[i]\n",
    "    return roc_auc,fpr,tpr     \n",
    "\n",
    "roc_auc,fpr, tpr=get_auc_values(L,Pre1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Ploting AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ploting_AUC(fpr, tpr, label_texts): \n",
    "\n",
    "  #%matplotlib inline\n",
    "    plt.figure()\n",
    "    lw=1\n",
    "    colors = (['aqua', 'darkorange', 'cornflowerblue','red','blue','maroon','coral','olive','aqua','springgreen','fuchsia','navy','plum','orchid','thistle'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                label='{0} (area = {1:0.2f})'\n",
    "                ''.format(label_texts[i], roc_auc[i]))\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('AUC for multi-class')\n",
    "      \n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "            fancybox=True, shadow=True, ncol=5)\n",
    "    plt.show()\n",
    "\n",
    "#ploting_AUC(fpr, tpr, label_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ploting_AUC(fpr, tpr, label_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Compute micro-average ROC curve and ROC area  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interp\n",
    "fpr_micro=dict()\n",
    "tpr_micro=dict()\n",
    "roc_auc_micro=dict()\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr_micro[\"micro\"], tpr_micro[\"micro\"], _ = roc_curve(LabelArray.ravel(), PredArray.ravel())\n",
    "roc_auc_micro[\"micro\"] = auc(fpr_micro[\"micro\"], tpr_micro[\"micro\"])\n",
    "\n",
    "# Find   macro-average metrics \n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr_macro = dict()\n",
    "tpr_macro = dict()\n",
    "roc_auc_macro= dict()\n",
    "fpr_macro[\"macro\"] = all_fpr\n",
    "tpr_macro[\"macro\"] = mean_tpr\n",
    "roc_auc_macro[\"macro\"] = auc(fpr_macro[\"macro\"], tpr_macro[\"macro\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('micro average := {0}'.format(roc_auc_micro[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('macro average :=  {0}'.format(roc_auc_macro[\"macro\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Plot macro and micro  curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.title('micro -averge') \n",
    "plt.plot(fpr_micro[\"micro\"], tpr_micro[\"micro\"]\n",
    ",label='micro-average ROC curve (area = {0:0.2f})' \n",
    "''.format(roc_auc_micro[\"micro\"]),\n",
    "         color='blue', linestyle=':', linewidth=2 )\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
    "            fancybox=True, shadow=True, ncol=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('macro -averge') \n",
    "plt.plot(fpr_macro[\"macro\"], tpr_macro[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc_macro[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.02),\n",
    "            fancybox=True, shadow=True, ncol=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=dict()\n",
    "accuracy=dict()\n",
    "p1=PredArray\n",
    "p2=p1>0.5\n",
    "p= p2.astype(int)\n",
    "y_scores=p\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "for i in range(n_classes):\n",
    "    \n",
    "    correct[i] =np.sum(LabelArray[:, i]==y_scores[:, i])\n",
    "    accuracy[i]=correct[i]/(len(LabelArray[:, i]))\n",
    "  \n",
    "\n",
    "for i in accuracy.iteritems():\n",
    "    avgDict = sum(i)/ float(len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total accuracy= {0}'.format(avgDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
